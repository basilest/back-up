AWS CLI

             install            pip3 install awscli --upgrade --user

             upgrade            (same of install)

             uninstall          pip3 uninstall awscli

     v2 on mac (both install & UPDATE):

                $ curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
                $ sudo installer -pkg AWSCLIV2.pkg -target /


                ╰─ curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg" ; sudo installer -pkg AWSCLIV2.pkg -target /

                      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                                     Dload  Upload   Total   Spent    Left  Speed
                    100 24.9M  100 24.9M    0     0   267k      0  0:01:35  0:01:35 --:--:--  324k
                    Password:
                    installer: Package name is AWS Command Line Interface
                    installer: Upgrading at base path /
                    installer: The upgrade was successful.

                    ╭─   28048   192.168.0.3  sbasile@pd12671   ~/CHL 
                    ╰─

----------------------------------------------------------
AWS CLI --debug
                           . aws --debug ....    # add whenever a command doesn't work as expected

                                                 ex. used with  aws --profile live --debug sqs list-queues
                                                     to find that python was failing to work with its module 'boto'
----------------------------------------------------------
AWS CLI configure
                           . aws configure      <--- this will create a default entry to use
                                                     aws configure --profile user2   <--- this instead will add a specific user2 entry


                            AWS Access Key ID [****************EPKQ]: AKIATU4B3DTQIYZRRPH2
                            AWS Secret Access Key [****************Y92B]: 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
                            Default region name [eu-west-1]: eu-west-2
                            Default output format [json]: json

                            aws sts get-session-token --duration-seconds 3600 --serial-number arn:aws:iam::250991044064:mfa/Stefano_Basile_Preprod --token-code 351747
                            {
                                "Credentials": {
                                    "AccessKeyId": "ASIATU4B3MXQFXJIYAHM",
                                    "SecretAccessKey": "I3otrXC4WSTkFmU9Tx8MRMb7sAeDrzxhJ4yvEM8B",
                                    "SessionToken": "FQoGZXIvYXdzENz//////////wEaDC/zrNSZVRgMp7O7MyKvAc5rbrx+9/itszJk0RsvM49LjX5P8U9nGFTbpMOYhgCjS9iKqqHF6PK5zJMposTJJgJbUKIp4nos/Yyzh5u0Wh5wPdPXw3PlabjhNqU2kHB/oGXO9gy3q58hVdZK8Dkafsn0ZQQtXQchdsROm74JV//x92NRyiR6VCBNkJATT1YH/CFY1Vo4/Np5xUQcxVZVxEVfz24Rlkqi3SPnooi8PGz/21ReIXbkjRELuS8JbMEo+PTH6AU=",
                                    "Expiration": "2019-06-25T11:42:00Z"
                                }
                            }

----------------------------------------------------------
MFA token  &  AWS CLI

                 - must create a temporary SESSION token

                    values similar to
                           . GAHT12345678                                                 (on hardware devices)
                           . the value is similar to arn:aws:iam::123456789012:mfa/user.  (on virtual MFA)









----------------------------------------------------------
aws-cli for S3

                 these commands assume the wanted  AWS_PROFILE  (i.e. export AWS_PROFILE=staging)
                 addresses a profile (i.e. staging) actually listed in both

                           1 ~/.aws/config
                           2 ~/.aws/credentials

								 If the AWS_PROFILE var is not defined or if I want to run a command with a profile
								 different from AWS_PROFILE I can use the --profile ....
											 ex.   aws ec2 describe-instances --profile user1

                 ~/.aws/config
                                        [profile development]
                                        output = json
                                        region = eu-west-1

                                        [default]
                                        region = eu-west-2
                                        output = json

                                        [profile default]
                                        output = json
                                        region =  eu-west-1

                                        [profile preprod]
                                        output = json
                                        region = eu-west-2

                                        [profile staging]   <------------- STAGING
                                        output = json
                                        region = eu-west-2

                                        [profile live]
                                        output = json
                                        region = eu-west-2
                 ~/.aws/credentials
                                        [sandbox]
                                        aws_access_key_id = IDAK..........ADUQFRIA
                                        aws_secret_access_key = 1eBYUXIdA....................xDD5JY29ndb

                                        [development]
                                        aws_access_key_id = AKIA..........D2EPKQ
                                        aws_secret_access_key = pGgzZvOn....................WbTpBCrgY92B

                                        [default]
                                        aws_access_key_id = AKIA..........C73QUS
                                        aws_secret_access_key = 4GlqV....................w3u/DrDlTFULdji

                                        [preprod]
                                        aws_access_key_id = AKIA..........C73QUS
                                        aws_secret_access_key = 4GlqV....................w3u/DrDlTFULdji

                                        [staging]                                                          <------------- STAGING
                                        aws_access_key_id = AKIA..........C73QUS
                                        aws_secret_access_key = 4Glq....................Ow3u/DrDlTFULdji

                                        [live]
                                        aws_access_key_id = AKIA..........4JL74A
                                        aws_secret_access_key = njNvl....................IUNNT569V/jbsbC



        aws s3 ls                            list   (note in aws terminology a 'dir' is a 'PREFIX', shortened as 'PRE')
        aws s3 ls help                       see the (few) options

                                                 ex. aws s3 ls s3://ch-service-staging-config.ch.gov.uk/staging/             <---- for a dir ending '/'  !!!
                                                 ex. aws s3 ls s3://ch-service-staging-config.ch.gov.uk/staging/global_env   <---- a file

        aws s3 mb s3://bucket-name           create a bucket    N.B. BUCKET NAMES MUST BE GLOBALLY UNIQUE AND SHOULD BE DNS COMPLIANT.

        aws s3 rb s3://bucket-name           delete a bucket     (it must be EMPTY)
        aws s3 rb s3://bucket-name --force   delete a bucket (content and subdirs)

        aws s3 rm                            as in unix   \
        aws s3 cp                                   "      |  they all come with the option '--grants'
        aws s3 sync                                 "     /   --grants Permission=Grantee_Type=Grantee_ID
                                                                            \           \          \_____________      <the value>
                                                                             \           \______________________    [uri emailaddress id]
                                                                              \_________________________________ [readacl writeacl full]

                ex.
                   aws s3 cp file.txt s3://my-bucket/ --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers full=emailaddress=user@example.com
                                                               ^^^^                                                     ^^^^    ^^^^^          ^^^^
                                                                                                                          1       2              3
       FILTERING:              --exclude  "<value>"         1 or + times (the last have precedence)
    (only  rm cp sync)         --include  "<value>"

					   *: Matches everything

					   ?: Matches any single character

					   [sequence]: Matches any character in sequence

					   [!sequence]: Matches any character not in sequence

				   ex:
					  --exclude  "*"      --include "*.txt"       All files excluded except .txt
					  --include  "*.txt"  --exclude "*"           All files excluded (even *.txt, so be careful in the order)



		   By default, ALL FILES ARE INCLUDED.

					   This means that an --include alone has no meaning.
					   --include has a meaning only when re-including files after a --exclude

					   so it's normally seen as

						  --exclude "*" --include "*.jpg" --include "*.txt"



       RECURSIVE:   --recursive

				   ex:
					  aws s3  cp  /tmp/foo/  s3://bucket/  --recursive



                TO LIST THE VERSIONS OF DOCS DOWN A VERSIONED BUCKET:

aws s3api list-object-versions --bucket shared-services.eu-west-2.configs.ch.gov.uk
                                        ^^^^^^^^^^^^^^^^^^^^ 1 ^^^^^^^^^^^^^^^^^^^^
                  1: bucket name (note:
                            - no s3://
                            - no / at the end
                            - it's not the dir where a file is present, it's really the bucketname,
                              the file is identified by its "Key" field)

                  the above command will dump a list of any versioned file in any sub-dirs, ex of list-entry:
                            {
                                "ETag": "\"b1714689211011a833081174ca2dfcd5\"",
                                "Size": 13334,
                                "StorageClass": "STANDARD",
                                "Key": "chl-xmlgw-configs/staging/XMLGWConfig.pm", <------------- the Key
                                "VersionId": "WzU_VHBe7som04kpW.OPuzya8BXvuX6d",   <------------- the version-id
                                "IsLatest": true,
                                "LastModified": "2021-06-09T09:20:05+00:00",
                                "Owner": {
                                    "ID": "98d4e194677d46e956f7af0d3c0e4f48a85aba407a365e005c647629b196550f"
                                }
                            },
aws s3api get-object --bucket shared-services.eu-west-2.configs.ch.gov.uk --version-id WzU_VHBe7som04kpW.OPuzya8BXvuX6d --key chl-xmlgw-configs/staging/XMLGWConfig.pm output_file
                              ^^^^^^^^^^^^^^^^^^^^ 1 ^^^^^^^^^^^^^^^^^^^^              ^^^^^^^^^^^^^^^ 2 ^^^^^^^^^^^^^^       ^^^^^^^^^^^^^^^^^^ 3 ^^^^^^^^^^^^^^^^^^  ^^ 4 ^^^^^^
                  1: bucket name: as for the "list-object-versions" command above
                  2: the version I want
                  3: the Key (it's usually a FULL path, so that -1- can just be a bucket name)
                  4: the output file where to save the result

    NOTE:
           4 (output_file) will contain the result, nonetheless on stderr is printed something like this:
                        {
                            "AcceptRanges": "bytes",
                            "LastModified": "2021-06-09T09:20:05+00:00",
                            "ContentLength": 13334,
                            "ETag": "\"b1714689211011a833081174ca2dfcd5\"",
                            "VersionId": "WzU_VHBe7som04kpW.OPuzya8BXvuX6d",
                            "ContentType": "binary/octet-stream",
                            "Metadata": {}
                        }


                TO SEARCH A DOC IN ALL S3 BUCKETS (by matching in the name or date)
-----------------

for b in $(aws --profile dev s3 ls s3:// | awk '{print $3}'); do echo "----------------checking [$b]"; aws --profile dev s3api list-objects --bucket $b --query "Contents[?contains(LastModified, '2023-05-18')]"; done
for b in $(aws --profile dev s3 ls s3:// | awk '{print $3}'); do echo "----------------checking [$b]"; aws --profile dev s3api list-objects --bucket $b --query "Contents;(LastModified, '2023-05-18')]" done
----------------------------------------------------------
aws-cli Session Manager

        sms

        I can install th optional plugin to ease connections to EC2

        INSTALL

        1. curl "https://s3.amazonaws.com/session-manager-downloads/plugin/latest/mac/sessionmanager-bundle.zip" -o "sessionmanager-bundle.zip"
        2. unzip sessionmanager-bundle.zip
        3. sudo ./sessionmanager-bundle/install -i /usr/local/sessionmanagerplugin -b /usr/local/bin/session-manager-plugin
                                                   ^^^^^^^^^^^^^^^ 1 ^^^^^^^^^^^^     ^^^^^^^^^^^^^^^^^ 2 ^^^^^^^^^^^^^^^^^

           I need Python 2.6.5 or later, or Python 3.3 or later.
           I can specify whoch python to use adding its absolute path (after sudo):

           3.1. sudo /usr/local/bin/python3.9 sessionmanager-bundle/install -i /usr/local/sessionmanagerplugin -b /usr/local/bin/session-manager-plugin
                     ^^^^^^^^^^^^^^^^^^^^^^^^                                  ^^^^^^^^^^^^^^^ 1 ^^^^^^^^^^^^     ^^^^^^^^^^^^^^^^^ 2 ^^^^^^^^^^^^^^^^^

              -i  (1)   the installed binary
              -b  (2)   a symlink to (1)

              -h to see the help      ./sessionmanager-bundle/install -h

        UNINSTALL

                       sudo rm -rf /usr/local/sessionmanagerplugin                (1)
                       sudo rm /usr/local/bin/session-manager-plugin              (2)


        VERIFY INSTALL

               $ session-manager-plugin

                 The Session Manager plugin was installed successfully. Use the AWS CLI to start a session.

               $ aws ssm start-session --target i-07e9bd6d3497545cd --region eu-west-2

        TUNNEL SSH via ssm
               1. Add this into ~/.ssh/config

                     # SSH over Session Manager
                     host i-* mi-*
                     ProxyCommand sh -c "aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters 'portNumber=%p'"

               2. ssh ec2-user@i-07e9bd6d3497545cd

----------------------------------------------------------
aws-cli SSO              (Single  Sign-On Portal)

╰─ aws configure sso
SSO start URL [None]: https://companies-house.awsapps.com/start#/
SSO Region [None]: eu-west-2
There are 9 AWS accounts available to you.
  ch-heritage-dev, heritagedev@companieshouse.gov.uk (300288021642)
  ch-shared-services, sharedservices@companieshouse.gov.uk (416670754337)
  ch-heritage-dev, heritagedevelopment@companieshouse.gov.uk (898467161089)
  Companies House - LIVE, awslive@companieshouse.gov.uk (449229032822)
  CH PreProd, awspreprod@companieshouse.gov.uk (250991044064)
  ch-innovation, awsinnovation@companieshouse.gov.uk (450865997890)
  ch-heritage-staging, heritagestaging@companieshouse.gov.uk (973538526606)
  Companies House, mfairhurst@companieshouse.gov.uk (169942020521)
> ch-heritage-live, heritagelive@companieshouse.gov.uk (976051798167)



                                      ╰─ aws configure sso
                                      SSO start URL [None]: https://companies-house.awsapps.com/start#/
                                      SSO Region [None]: eu-west-2
                                      There are 9 AWS accounts available to you.
                                      Using the account ID 976051798167
                                      The only role available to you is: PowerUserAccess
                                      Using the role name "PowerUserAccess"
                                      CLI default client Region [eu-west-1]: eu-west-2
                                      CLI default output format [json]: json
                                      CLI profile name [PowerUserAccess-976051798167]: hlive

                                      To use this profile, specify the profile name using --profile, as shown:

                                      aws s3 ls --profile hlive


----------------------------------------------------------
Why AWF:  It is the FASTEST growing cloud computing platform



AWS  SERVICES    2011:     82
                 2012:    159
                 2013:    280
                 2014:    516
                 2015:    735
                 2016:  +1000
|________________________________________________________|
|TECHNOLOGY PARTNER:   each different technology         |
|                      to   I N T E R A C T  to AWS      |  AWS-Partner-Certifications <----
|                                                        |
|CONSULTING PARTNER:   ...Accenture Datapipe             |
|________________________________________________________|
|                                                        |
|
      To be a PARTNER  a company
      must have a proper number of CERTIFIED EMPLOYEE




      ASSOCIATE        (SOLUTIONS ARCHIT.     DEVELOPER       SYSOP ADMIN
                                                   |           |
      PROFESSIONAL           "                    DEVOPS-PROFES.)

      SPECIALITY          SECURITY      ADVANCED NETWORKING      BIG DATA





     . AWS  FUNDAMENTALS
     . DESIGN  & DEVELOP
     . DEPLOYM & SECURITY
     . DEBUGING



       AWS PLATFORM:  1. Messaging
                      2. Security & Identity
                      3. Management Tools
                      4. Storage
                      5. Databases
                      6. Networking & Content Delivery
                      7. Compute
                      8. AWS Global Infrastructure: REGIONS / AVAILABILITY ZONES / EDGE LOCATIONS



             8. AWS Global Infrasructure:

                      REGION         = Geog. AREA of 2+ AVAILAB. ZONES
                      AVAILAB. ZONES = Data Center
                      EDGE LOC       = They CACHE the contet (i.e. a video from Australia
                                                              requested by a 1st user in NEW YORK
                                                              is actually transferred across.
                                                              A 2nd NEW YORK user will find it cached)
                                       Said otherwise EDGE LOC are
                                       Content-Delivery-Network (CDN)  End Point for CloudFront

                           . EDGE LOCs >> REGIONs
                           . Not all REGIONs have ALL the services

             6. Networking & Content Delivery:
                                                . VPC: Virtual Private Cloud
                                                  (think of it as a Virtual DATA CENTER)

                                                  They can be >1 in a REGION


                                                . Route53: Amazon DNS service
                                                           (nb. 53 is DNS standard port)

                                                . CloudFront: The Cache service made by many
                                                              EDGE locat.

                                                . Direct Connect: Connect my office to AWS
                                                              with a physical dedicated machine (and line)
                                                              (for security reason)

             7. Compute
                                                . EC2: they are the VM (like Virtualbox)
                                                       So think of EC2 as the service for Virtualization

                                                       It's implemented by Docker container called:
                                                     - EC2 Container service

                                                     - Elastic Beanstalk: when uploading to it any piece of code
                                                                          it sets up the required infrastructure
                                                                          to run the code

                                                     - Lambda: the flexible service behind all

             4. Storage
                                                4 components:

                                                1 S3 (old as AWS, it stays for 3 S: Simple Storage Service)
                                                                   think of it as a VIRTUAL DISK (DropBox)
                                                                   in the cloud to store OBJECTS
                                                                   (any file / format [not DB])

                                                2 GLACIER  low cost service to ARCHIVE files (ex for law
                                                           you must keep files for 7 years)

                                                3 EFS new service (Elastich File Service) this is where
                                                      (instead of S3) you install DB and applications (ex. a game)

                                                4 Storage GATEWAY:  a VM image that must be installed
                                                  in my Company's premises to connect to S3.



             5. Databases
                                                . RDS (Relational DB Service) includes all the main ones
                                                  Oracle, MySQL, ...     <--- the relational ones

                                                . DynamoDB (for the NON-Relational DB, or No-SQL)
                                                  very scalable (& high performance)

                                                . Redshift (Amazon warehousing) where to store copies
                                                  of my Production DB, to perform calculation, statistics, measures
                                                  OUT-FROM my production env (whose perform. would otherwise suffer)

                                                . ElastichSearch (Cache for Redshift) ex to store the top-10
                                                  sold items or other very used info (better ito retrieve
                                                  from here, than each time from DB)




             1. Migration
                                                . Snowball : a physical appliance (factor size = a suitcase)
                                                             to transfer physical DISKs to Amazon
                                                             and it will store them into S3 or EFS

                                                . DMS (DB Migration Service) to transfer my DB to the cloud
                                                  or then to move to different REGIONS
                                                  The good thing is that it can convert to another format
                                                  (ex. an ORACLE DB to a MySQL DB)

                                                . SMS (Server Migration S.) to migrate a VM (ex VMware)
                                                  to the Cloud.



             1. Analytics
                                                . Athena: new service (2017) to run SQL query on S3

                                                . EMR  (Elastich

                                                . Cloud   Search  : they do the same just Elast. S is an
                                                . Elastic Search  : (open source) framework

                                                . Kinesis: realtime analysis (it analyse tera bytes / h)
                                                           it's a stream monitor

                                                . Data pipeline (like the UNIC pipe) moves data from one
                                                  service to another

                                                . Quick Sight: Buiseness analytic tool, with visualisation
                                                               and a dashboard


             2. Security & Identity
                                                . IAM: fondamental component of AWS (present anywhere)

                                                . Inspector: a Service to install in my VMs to scan and
                                                             monitor what's going on.

                                                . Directory Service: to use active directories (connect
                                                  Microsoft to AWS)

                                                . WAF (Web Application Firewall)

                                                . Artifacts: to manage certifications (ISO....) docs



             3. Management Tools
                                                . Cloud Watch: to monitor performances

                                                . Cloud Formation: fondamental topic, it allows to deploy
                                                  my AWS env designing my virtual devices (routers, firewalls...)
                                                  with writing some TEMPLATES or even 1 SINGLE LINE of CODE.

                                                . Cloud Trail:

                                                . Opsworks: to automated deployment

                                                . Config: monitor my env. and warns if my config is not OK
                                                          (even at runtime, ex some users are acting against
                                                           some policies)

                                                . Service Catalog

                                                . Trusted Advisor




             1. Application Services
                                                . Step Functions: new (2017)

                                                . SWF

                                                . API Gateway: a door to enter my BackeEnd services

                                                . AppStream: to stream desktop App to my user.

                                                . Elastic Transcoder: to transcode avideo in different format
                                                                      (ex desktop, mobile, ...)




             1. Developer Tools
                                                . Code Commit: like GitHub

                                                . Code Build: to compile my code in different env (cross compile)

                                                . Code Deploy: to deploy code to my EC2

                                                . Code Pipeline: to keep trac of different version of code
                                                                 (testing, live, preprod, ...)


             1. Mobile Services
                                                . Mobile Hub: to configure my App if it's a mobile one
                                                              (Backend, data storage, user auth, ...)
                                                              It has an own Console, not the AWS common one.

                                                . Cognito: to log into my App through social id
                                                           (facebook, google, ...)

                                                . Device Farm: to test my App on hundreds of different HW-OS
                                                                (Android, IOs, ...)

                                                . Mobile Analytics: to monitor data

                                                . Pin point: new (2017) to use like google Analytic for my App
                                                             trace user behaviour ...




             1. Business Productivity
                                                . Work Docs: to store docs in a safe place.

                                                . Work Mail: to send & receive emails


             1. Internet of Things
                                                . iOT: new (2017) probably a self Certification


             1. Desktop - App streaming
                                                . Work Spaces: VDI (like having my Desktop in th Cloud)

                                                . AppStream 2.0: to stream my Desktop-App to my user

                                                . Work Spaces: VDI



             1. Artificial Intelligence
                                                . Book from Nick Bostrom
                                                       SUPERINTELLIGENCE

                                                . Alexa
                                                       use 'lex' driver
                                                       use 'Polly' translates text into mp3 audio
                                                       'Machine learning' to predict data according to
                                                                          an input data-set
                                                       'Rekognition' understand what is present in a picture


             1. Messaging
                                                . SNS
                                                . SQS: a queue system (I can store jobs in a queue)
                                                . SES: Simple Email Service


             8. AWS Global Infrasructure:





_________________________________________________________ Identity Access Management - (IAM)

               It provides:

             . CENTRALIZED control of my AWS
             . SHARED access (I can invite others)
             . GRANULAR Permissions
                        ex. John Smith can only use DinamoDB
             . Identity Federation (ie. Linkedin, facebook, ... to login)
             . Multi Factor Authent. (MFA) (like GitHub)  [can be virtual or physical]
             . Provide TEMP access (TEMP users)
             . Password ROTATION Policy (and even password chars-range/len constraints)
             . Support PCI DSS Compliance (for Credit Card / Paypal ...)


                  key terms:
                         . USER
                         . GROUP    = set of USERs
                         . ROLE     = when assigned to AWS services the can communicate
                         . POLICIES = A DOCUMENTS to define the PERMISSIONS

                           NEW Users have NO premission when first created
                           NEW Users when first created receive: ACCESS Key +            <--- they are not the user+password pair
                                                                 SECRET ACCESS Key            but are the tokens that can be used
                                                                 to log into the CONSOLE      when accessing (programmatically)
                                                                                              using the
                                                          they can be seen                              . APIs
                                                          only the first time                           . command line
                                                          (that is so the time to download/save them)
                                                          because otherwsie you need to
                                                          create them again


             SAML : Secure Assertive Markup Language (the auth. used to access Windows Active Directory)


             Windows Active dir. is accessed FIRST, and then I get a TEMPORARY access credential to AWS

_________________________________________________________ Web Identity Federation (with Mobile App)

             Web Identity Federation Playground : https://web-identity-federation-playground.s3.amazonaws.com/index.html


                 1. I log in a web account (ex. Facebook) (called the "Identitiy  provider")
                 2. I receive a TOKEN
                 3. setting the token in my policy,  <----  I see the countdown in sec of my TOKEN expiration
                    I can receive temporary credentials
                    clicking on "Call AssumeRoleWithWebIdentity" <--- where the 'Role'
                                                                      is ARN
                                                                      arn:aws:iam::877950674958:role/WebIdFed_Facebook


                    ex. of Token:
                                    accessToken: EAAIZCWusQngMBAB1q...zsE
                                    userID: 1721332184847663
                                    expiresIn: 5086



                    I see the request message:
                                                  GET / HTTP/1.1
                                                  Host: sts.amazonaws.com
                                                  Content-Type: application/json; charset=utf-8
                                                  URL: https://sts.amazonaws.com/?Pr


                    and the response
												<AssumeRoleWithWebIdentityResponse
												  xmlns="https://sts.amazonaws.com/doc/2011-06-15/">
												  <AssumeRoleWithWebIdentityResult>
													<Audience>632609676762627</Audience>
													<AssumedRoleUser>
													  <Arn>arn:aws:sts::877950674958:assumed-role/WebIdFed_Facebook/web-identity-federation</Arn>
													  <AssumedRoleId>AROAJJXNICVEOTE5KRC2W:web-identity-federation</AssumedRoleId>
													</AssumedRoleUser>
													<Provider>graph.facebook.com</Provider>
													<Credentials>
													  <AccessKeyId>ASIAJ5KKX6AZ4UBCTXCQ</AccessKeyId>
													  <SecretAccessKey>83CfHgfQnkTCxxfVWoCks+d6AfxBGBmm57WfhthD</SecretAccessKey>
													  <SessionToken>FQoDYXdV...............................................................
													  <Expiration>2017-01-17T22:53:07Z</Expiration>
													</Credentials>
													<SubjectFromWebIdentityToken>1721332184847663</SubjectFromWebIdentityToken>
												  </AssumeRoleWithWebIdentityResult>
												  <ResponseMetadata>
													<RequestId>9dddc325-dd05-11e6-8095-87e3e4c47226</RequestId>
												  </ResponseMetadata>
												</AssumeRoleWithWebIdentityResponse>



_________________________________________________________ S3 - EC2   (Elastic Compute Cloud)

             I can provision my virtual server with what I need
             and the scale with ease (if I need more capacity)
             I pay EC2 according to how many resources I ask

                     I can pay:
                         - ON DEMAND:  counting how many hours
                         - RESERVED:   I pay for 1 or 3 years (discount)
                         - SPOT:       cheapest (I can be shutdown with 1 hour notice
                                                 if I exceed the discounted agreed usage)
                                                 This model can be good if my App is something
                                                 that starts & requires a lot of resources only
                                                 sometimes (a big calculation only when some data arrives)
                                                 then can be shutdown.

                                                 nb. If it's me to shutdown I'll pay for the time it runs
                                                     otherwise if it's Amazon to shutdown I'll not pay.


                           'ON DEMAND' can be added to 'RESERVED' if needed
                           (ex on a black Friday I need more resources
                            I pay just for that day my required extra
                            than I turn back to normal 'RESERVED')

             Families (
                         T2       General Purpose (cheap)     webser/small DB
                         M4, M3    "                          Application server
                         C4, C3   CPU Intensive
                         R3       Memory intensive            DB or App
                         G2       Graphic (Video encoding)
                         I2       High Speed                  (NOSQL DB)
                         D2       Dense Storage               Files server
                      )

             EBS : is like a HARD-DISK (a block device) in the cloud (so replicated from failure)
                   It allow me to create virtual 'storage volumes' and attach to 1 EC2  (to mount to >1EC2 --> use EF)
                   then:
                           . create a file-system on top
                           . run a DB
             Families (
                         SSD (GP2)  99.9999% availability
                                    <= 10.000 IOPS

                         IOPS SDD (IO1)  IOPS (Iput-Output.Per-Second)
                                    if I need > 10000 IOPS  (ex. NOSQL DB)

                         MAGNETIC (Standard)
                                    lowest cost per Giga
                      )


_________________________________________________________ EC2

             . Log into Console
             . Choose EC2
             . I can Choose my best region
             . I can see the status of Region and Availability Zones
             . Then I can create a EC2 from some defaults (Suse, RedHat, Ubuntu (anche Windows!)
                                                          and a Linux version from Amazon.)

             . Some EC2 are HVM (Hardware Virtual Machine)
                 others are PV  (Para Virtual)

             . EC2 are presented with the family id. (T2, M4, C4, ...)
               how much RAM and how many CPUs
               I can even choose the network params (IP/subnet mask [1 subnet = 1 Availability Zone])
               and If I want to open/attach a SPOT account

             . In the advanced section I can enter in a text-box some config script commnds
               ex.
                    #!/bin/bash
                    yum update -y

             . It's FREE to create simple EC2 (ex. 8 GB & 24 IOS)
             . The defaults of the ROOT Volume (where the OS is stored) are:
                               - "Delete on Termination" flag = ON, so the FREE EC2 by default are deleted on close.
                               - Encryption is OFF (and it grayed so it CANNOT be set ON) there are 3rd party tools
                                 to add the Encryption tough

             . I can add other volumes (not only the ROOT). Ex 1 EBS
               These volumes have the flag to Encrypt that is available

             . I can add TAGS (in the usual HASH forms (KEY:VALUE pair)
               ex Developer:TEST

             . I can add security policy (a virtual firewall)
               ex define SSH  on port 22 to access everywhere +
                         HTTP on port 80
               (both on TCP)



             . At the end, LAUNCH the new EC2


             . If I download the public key (file xxxxx.pub) I can
               log from my laptop-terminal as:

                     1. chmod 600 xxxxx.pub
                     2. ssh basilest-amazon@52.48..78.87 -i xxxxx.pub




_________________________________________________________ Windows users (configure Putty & SSH)

                                Putty.exe       +
                                Puttygen.exe



                   AWS lets you
                     .  create  a SSH key pair (private & public)
                     .  download the public as a file  xxxxxx.pem

                   Putty is not able to manage the .pem format so that
                   there is the Puttygen.exe to convert in a xxxxx.ppk






_________________________________________________________ Configure AWS command line


                  to use "User-Credentials" to log
                  With ROLEs is more secure.

            1. Create a Amazon-Linux-EC2   (becasue the AWS-command-line-tool is already included)
            2. Create a User  uuuu
            3. Create a Group
            4. Attach the User to the Group
            5. Attach 1 of up to 10 Policies to the Group
               (1 good is: "AWS Full Access")
            6. from your laptop's terminal:
               ssh uuuu@1.2.3.4 -i xxxxx.pem
                          ^          ^
                          |        the public AWS key saved
                          |________the IP addr. of the EC2







_________________________________________________________ AWS commands


            They all
                 .  start as 'aws'
                 .  the 'service' follows (ex s3
                 .  1 of the many comanfds availabke for that 'service'
                ex
                 > aws s3 ls


            aws configure         to configure my EC2
                                  the 1st thing to configure is to add the
                                  'Acess key ID'  (the 1st of the 2 voices saved when creating the EC2)
                                  and they even 'Secret Access key'



                                  Then I can set:
                                     . region   (ex. us-east-1)
                                     . default output format (ex. JSON)


            aws s3 mb s3://sdfsdfsdf      to create a bucket

            .aws                           the usual hidden dir, containing
                                             > config       <--- contains the other configure settings (ex. region:us-east-1)
                                               credentials  <--- contains the 2 'Access' infos
                                                                                  ^
                                                                                  SO USING THEM IS VERY INSECURE!!!!!!
                                                                                  BETTER TO USE OTHER WAYS THAN
                                                                                  USING THE 'Access key'


_________________________________________________________ ACCESS WITH ROLE SETTINGS

               ROLES can be attached
                   O N L Y
               when the EC2 is   C R E A T E D (or provisioned)


               So the steps are:

                  1. create a ROLE
                  2. attach the AWS-S3-FULL-ACCESS Policy
                  3. Create an EC2 :
                                   3.1   at the 2nd page in the 'IAM Role' field
                                         select the ROLE just created.
                                   3.2   some pages then attach 1 GROUP previously
                                         created as 'security group'



                    The EC2 created in that way
                    DO NOT STORE the credentials!! (So there are no files in .aws dir)


_________________________________________________________ PHP - SDK

            Script to run in the EC2:

            |
            |  yum update -y
            |  yum install httpd24 php56 git -y                 httpd24 is Apache 2.4
            |  service httpd start                              start Apache
            |  chkconfig httpd on
            |  cd /var/www/html
            |  echo "<?php phpinfo();?>" > test.php
            |  git clone .....
            |

               To install the SDK for Php  (that is a AWS Service)
               run this:
                        ssh uuuu@1.2.3.4 -i xxxxx.pem
                        sudo su
                        cd /var/www/html

                         curl -sS https://getcomposer.org/installer | php

                         php composer.phar require aws/aws-sdk.php

                        the previous cmd creates some dirs (ex. /var/www/html/ vendor/
                            cd vendor
                            run (each time you want to open the Php-SDK),
                            the file
                                       autoload.php




_________________________________________________________ Interacting with S3  (using Php SDK)


                        ssh uuuu@1.2.3.4 -i xxxxx.pem
                        cd /var/www/html/ s3/ *.php    <----------------------------------------------------------
                            1. the first file is createbucket.php :                                                \
                                                                                                                     \
              ___________________  include 'connecttoaws.php'   (nb. its connect-to-aws, and include another file of these)
              |                    $bucket = uniqid("cloudGURU", true)
              |                    $result = $client->createBucket(array('Bucket' => $bucket));
              |                                                ...
              |
  contains    |
  at least    |
  these lines |                    Once the bucket is created it is passed to the other pages like that:
              |                            <a href=\"xxxxxx.php?bucket=$bucket\">
            },
            {
                "Grantee": {
                    "ID": "63493296d0bdddaacd95a4db53b41720c47d32f4eabbdf790b962208d5347b5d",
                    "Type": "CanonicalUser"
                },
                "Permission": "WRITE"
            },
            {
                "Grantee": {
                    "ID": "63493296d0bdddaacd95a4db53b41720c47d32f4eabbdf790b962208d5347b5d",
                    "Type": "CanonicalUser"
                },
                "Permission": "READ_ACP"
            },
            {
                "Grantee": {
                    "ID": "63493296d0bdddaacd95a4db53b41720c47d32f4eabbdf790b962208d5347b5d",
                    "Type": "CanonicalUser"
                },
                "Permission": "WRITE_ACP"
            }
        ]
    }
_________________________________________________________ POLICY TO ENFORCE S3 TO MANAGE TLS > 1.2  (on a bucket named: "ch-document-api-chips-staging")
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "EnforceTLSv12orHigher",
			"Effect": "Deny",
			"Principal": {
				"AWS": "*"
			},
			"Action": "s3:*",
			"Resource": [
				"arn:aws:s3:::ch-document-api-chips-staging/*",
				"arn:aws:s3:::ch-document-api-chips-staging"
			],
			"Condition": {
				"NumericLessThan": {
					"s3:TlsVersion": "1.2"
				}
			}
		}
	]
}
_________________________________________________________ Key pairs


   1. The keys are used to access EC2 (over ssh)
   2. I can see their fingerprints in the console in EC2 / Network & Security / Key Pairs

         Name   Fingerprint                                      ID
	     ewf	06:07:86:a1:d0:44:4c:32:99:e7:3e:dc:27:3b:da:b9	key-025b7a0195275f6a7

   3. The keys as file .pem CANNOT be seen again after it's created.
      The shown "Fingerprint" is just a DER-encoded copy of the private key.
      (just for veryfing that it's from the key I have). So if the pem key is lost
      it must be created another one

   4. $ aws ec2   describe-key-pairs                         # to see the pairs
   5. $ aws ec2   describe-key-pairs --key-name ewf          # to shown only that pair
   6. $ aws ec2   delete-key-pair    --key-name ewf          # to delete
   7. $ aws ec2   create-key-pair    --key-name ewf --query 'KeyMaterial' --output text > ewf.pem

   8. put the key in ~/.ssh with chmod 400






