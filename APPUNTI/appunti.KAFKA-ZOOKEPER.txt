



--------------------------------------------------------------------------------
    cd /etc/kafka <--------------- usually the dir where the config is

     exemple of its content:

        -rw-r--r-- 1 root root  906 Aug 26  2016 connect-console-sink.properties
        -rw-r--r-- 1 root root  909 Aug 26  2016 connect-console-source.properties
        -rw-r--r-- 1 root root 2760 Aug 26  2016 connect-distributed.properties
        -rw-r--r-- 1 root root  883 Aug 26  2016 connect-file-sink.properties
        -rw-r--r-- 1 root root  881 Aug 26  2016 connect-file-source.properties
        -rw-r--r-- 1 root root 1074 Aug 26  2016 connect-log4j.properties
        -rw-r--r-- 1 root root 2061 Aug 26  2016 connect-standalone.properties
        -rw-r--r-- 1 root root 1199 Aug 26  2016 consumer.properties
        -rw-r--r-- 1 root root 4369 Aug 26  2016 log4j.properties
        -rw-r--r-- 1 root root 1900 Aug 26  2016 producer.properties
        -r--r--r-- 1 root root 6384 Feb 20  2019 server.properties
        -rw-r--r-- 1 root root 1032 Aug 26  2016 tools-log4j.properties
        -rw-r--r-- 1 root root 1027 Aug 26  2016 zookeeper.properties

ex. ito see some config (ex. retention period)

    [centos@kafka-streaming-broker1 kafka]$ grep -i retention *
    server.properties:############################# Log Retention Policy #############################
    server.properties:log.retention.hours=168
    server.properties:# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
    server.properties:# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
    server.properties:#log.retention.bytes=1073741824
    server.properties:# to the retention policies
    server.properties:log.retention.check.interval.ms=300000
    server.properties:# By default the log cleaner is disabled and the log retention policy will default to just delete segments after their retention expires.
    [centos@kafka-streaming-broker1 kafka]$

so the deafult 168 hours (1 week)
--------------------------------------------------------------------------------
ZOOKEPER
     LIST REGISTERED BROKERS
                                            1. ssh -i ~/.ssh/ch-aws-staging.pem centos@kafka-streaming-zk1.staging.aws.internal
                                            2. zookeeper-shell localhost:2181 ls /brokers/ids
--------------------------------------------------------------------------------

     MORE ZOOKEPERS:    comma ',' separated values of <address : port>

                        --zookeeper kafka-zk1.live.aws.internal:2181,kafka-zk2.live.aws.internal:2181,kafka-zk3.live.aws.internal:2181



     MORE BROKERS:      comma ',' separated values of <address : port>

                        --broker-list kafka-broker1.live.aws.internal:9092,kafka-broker2.live.aws.internal:9092,kafka-broker3.live.aws.internal:9092


     LIST ALL TOPICS:
                        bin/kafka-topics.sh  --list --zookeeper localhost:2181
                        kafka-topics  --list --zookeeper 10.53.21.234:2181        :in AWS (
                                                                                      1.  ssh into the kafka instance:
                                                                                          ssh -i ~/.ssh/chs-fozzie.pem  ec2-user@10.53.22.10
                                                                                      2. sudo su
                                                                                      3. get the IP:PORT for zookeeper server
                                                                                         kafka-topics  --list --zookeeper 10.53.21.234:2181

                        kafka-topics --list --zookeeper kafka-streaming-zk1.staging.aws.internal:2181

     GET INFO ON ALL TOPICS         kafka-topics --zookeeper 10.65.2.30:2181 --describe
     GET INFO ON 1 TOPIC            kafka-topics --zookeeper 10.65.2.30:2181 --describe --topic resource-changed-data

                                    output:

                                     Topic:resource-changed-data     PartitionCount:3        ReplicationFactor:3     Configs:
                                             Topic: resource-changed-data    Partition: 0    Leader: 2       Replicas: 3,1,2 Isr: 2,1
                                             Topic: resource-changed-data    Partition: 1    Leader: 2       Replicas: 2,1,3 Isr: 2,1
                                             Topic: resource-changed-data    Partition: 2    Leader: 2       Replicas: 3,2,1 Isr: 2,1


     SHOW/CONSUME MESSAGES FOR A TOPIC:
                        kafka-console-consumer  --zookeeper kafka-zk1.live.aws.internal:2181,kafka-zk2.live.aws.internal:2181,kafka-zk3.live.aws.internal:2181 --topic order-received --from-beginning

     WRITE/PRODUCE MESSAGES ON A TOPIC:
                        export TOPIC_NAME=xxxxxx
                        kafka-console-producer --broker-list kafka-broker1.live.aws.internal:9092,kafka-broker2.live.aws.internal:9092,kafka-broker3.live.aws.internal:9092 --topic ${TOPIC_NAME}

     COPY MESSAGES FROM AN OFFSET --> TO THE FRONT OF THE QUEUE:
                        # there isn't a single command , expecially in old version of Kafka, so it could be done in 3 steps

                        1. step:  dump the messages in a text file  (see SHOW/CONSUME MESSAGES FOR A TOPIC) >& file.txt
                        2. edit the file.txt leaving only the messages to copy to the front
                        3. for i in $(cat file.txt ); do echo "------------ processing [$i]----"; printf $i |   (see WRITE/PRODUCE MESSAGES ON A TOPIC)


     ADD 1 PARTITION TO 1 TOPIC
                        kafka-topics --zookeeper 10.65.2.30:2181 --alter --topic resource-changed-data --partitions 4

     SEE TOPIC CHANNEL
                        bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic $TOPIC --from-beginning
                        bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic $TOPIC --from-beginning

     SEND MESSAGE TO ERIC
                      curl -u OpXFFV77ghkgZlK9U7HcVvaK0rUqKUK0M0AlDi5c: http://10.53.21.119:31839/filings\?timepoint=2
--------------------------------------------------------------------------------


     KAFKA       must be thought like a black-box or layer
                 which is used to send messages on 1 side
                 and read them out on the other side:


                 ------>                          ------>
                 ------>   ------------------     ------>
                 ------>   |    K A F K A   |     ------>
                 ------>   ------------------     ------>
                 ------>                          ------>

                 Actually this black-box is:

                     - Distributed     so not 1 single point of failure
                                       (it's redundancy deployed as more clusters)

                     - Scalable        I can extend it, adding more resorces, if needed



                 Data written to Kafka is written to disk and replicated for fault-tolerance.

     ACKS        I can configure a wait for acknowledgement in the SEND so that a write isn't considered complete until
                 it is fully replicated and guaranteed to persist even if the server written to fails.




     TCP         the communication is done via TCP






     TERMS       Kafka defines these terms:
                                                                             msg
                    __ PRODUCERS         they send messages      PRODUCER   ----->  (TOPIC)
          CLIENTS _/                                                                           msg
                   \__ CONSUMERS         they read messages                         (TOPIC)   ----->   CONSUMER

                                         - CLIENTS are available in many languages, 1st is in JAVA


                     - TOPIC             messages are sent to a TOPIC, and are read from a TOPIC
                                         A topic is like a BUFFER where messages are enqueued ( push & pop )


                                         1 or + CONSUMERs can subsribe to a TOPIC  ( multi-subscriber )
                                         CONSUMERs of the same TOPIC are groupped into a CONSUMER-GROUP



                     - PARTITION         1 TOPIC = more PARTITIONS
                                         this allows to parallelize more push-s and pop-s

                                                       ===>  high message processing throughput.

                                         1 PARTITION stays phisically on 1 SERVER
                                         but different PRATITIONs of the same TOPIC can stay on different SERVERs



                     - OFFSET            Is the ID which identifies a message in 1 PARTITION
                                         - actually is like the SEEK in a file, which allows to retrieve
                                           in the same buffer/log/file different records from different CONSUMERs


                     - BROKER            each NODE in 1 CLUSTER
                                         1 BROKER holds more duplicated PARTITIONS, and only 1 of the PARTITION is
                                                    LEADER    (the messages are sent to this, and it forwards it also to all its FOLLOWERS)
                                                    FOLLOWER   they send ack to the Leader so it knows they are in sync


                             So the 3 fields:
                                               ( TOPIC, PARTITION, OFFSET )  identify  1 MESSAGE


                              ---------------------------------------------------------------------------------------------------------------
                              |       __________ __________ __________ __________ __________ __________ __________
                              |      |  msg 1   |  msg 2   |  msg 3   |  msg 4   |  msg 5   |  msg 6   |  msg 7   |      PARTITION 1
                        T     |      |__________|__________|__________|__________|__________|__________|__________|
                         O    |
                          P   |       __________ __________ __________ __________
                           I  |      |  msg 1   |  msg 2   |  msg 3   |  msg 4   |      PARTITION 2
                            C |      |__________|__________|__________|__________|
                              |
                              |       __________ __________ __________ __________ __________ __________
                              |      |  msg 1   |  msg 2   |  msg 3   |  msg 4   |  msg 5   |  msg 6   |      PARTITION 3
                              |      |__________|__________|__________|__________|__________|__________|
                              |
                              --------------------------------------------------------------------------------------------------------------






                                                   broker 1              broker 2             broker 3
                                                 __________            __________           __________                (L) Leader
                                                |  PART 0  |(L)       |  PART 0  |F        |  PART 0  |F               F  Follower
                                                |__________|          |__________|         |__________|

                                                 __________            __________           __________
                                                |  PART 1  |F         |  PART 1  |(L)      |  PART 1  |F
                                                |__________|          |__________|         |__________|

                                                 __________            __________           __________
                                                |  PART 2  |F         |  PART 2  |F        |  PART 2  |(L)
                                                |__________|          |__________|         |__________|







     DELETE MESSAGES:      . Message stay in the PARTITION ( even once consumed )
                           . KAFKA deletes OLD messages from the PARTITION after 'some' time ( configurable (ex. 1 day) called RETENTION PERIOD )
                           . if the CONSUMER was down, during that period (ex. 1 day) the messages are lost.




     PRODUCER STRATEGIES    3 strategies can be configured:
                                                               1. PRODUCER WAITS that all brokers are in sync before sending another msg

                                                               2. PRODUCER WAITS only Leader broker

                                                               2. PRODUCER does NOT WAIT




     CONSUMERR STRATEGIES    3 strategies can be configured:           (A)           (B)               (C)
                                                               1. commit offset / process msg / save in downstream channel:

                                                                  if CONSUMER crashes after (A) it will pick-up next time
                                                                  another MSG and the crashed one is LOST

                                                                       (A)           (B)               (C)
                                                               2. process msg / commit offset / save in downstream channel:   <------ BETTER

                                                                  if CONSUMER crashes after (A) it will pick-up next time
                                                                  the same MSG and the crashed one will be present twice in the downstream channel

                                                                       (A)           (B)                           (C)            (D)
                                                               3.  process msg / save in downstream channel / save a session / downstream channel:

                                                                  if CONSUMER crashes after (B) everything restarts from the saved
                                                                  SESSION.
                                                                  This approach guarantes NO DATA LOSS and NO DUPLICATION
                                                                  BUT it decreases the throughput, so 2. is the preferred solution, and
                                                                  it's up to the downstream channel to manage the duplications.



__________________________________________________  API

        4 API available to an application
                 Producer  API      the application can publish mes to a TOPIC
                 Consumer  API      the application can subscribe to 1 or more TOPICs & process the stream of messages
                 Streams   API      extension of the Consumer API when the application must tansform an INPUT stream into an OUTPUT stream
                                    In Kafka a stream processor is anything that takes continual streams of data from input topics,
                                    performs some processing and produces continual streams of data to output.
                 Connector API      to reuse PRODUCERs & CONSUMERs, lining to external resources (i.e. DB tables or external applications)



__________________________________________________  KAFKA MIRROR-MAKER

    Kafka MirrorMaker provides geo-replication support for your clusters.

    With  MirrorMaker, messages are replicated across multiple datacenters or cloud regions.

    You can use this in
        active/passive scenarios for backup and recovery;
        active/active  scenarios to place data closer to your users.
















__________________________________________________  ZooKeeper   (Apache)

   In the cloud erea, Autoscaler is a more robust solution.

   JAVA        It is an application (comes as a .JAR) which runs in the JAVA VM. (release 1.6 or greater)
               ( and actually logs with log4j  (version 1.2) )

               The Java HEAP size is very important to avoid swapping,
               which will seriously degrade ZooKeeper performance.
               Be conservative - use a size of 3GB for a 4GB machine.

                       - on the MAC:
                                    lrwxr-xr-x  1 sbasile  wheel  54 21 Jul 11:09 /usr/local/bin/zookeeper-security-migration -> ../Cellar/kafka/1.1.0/bin/zookeeper-security-migration
                                    lrwxr-xr-x  1 sbasile  wheel  48 21 Jul 11:09 /usr/local/bin/zookeeper-server-start       -> ../Cellar/kafka/1.1.0/bin/zookeeper-server-start
                                    lrwxr-xr-x  1 sbasile  wheel  47 21 Jul 11:09 /usr/local/bin/zookeeper-server-stop        -> ../Cellar/kafka/1.1.0/bin/zookeeper-server-stop
                                    lrwxr-xr-x  1 sbasile  wheel  41 21 Jul 11:09 /usr/local/bin/zookeeper-shell              -> ../Cellar/kafka/1.1.0/bin/zookeeper-shell


                                    they are small 1-liner files
                                    ex: ( /usr/local/bin/zookeeper-server-start )
                                       JAVA_HOME="$(/usr/libexec/java_home --version 1.8)" exec "/usr/local/Cellar/kafka/1.1.0/libexec/bin/zookeeper-server-start.sh" "$@"

                                    each calling a bigger bash file
                                    ex ( /usr/local/Cellar/kafka/1.1.0/libexec/bin/zookeeper-server-start.sh )


                       - on a vagrant VM
                                    /home/vagrant/zookeeper-3.4.11/bin
                                                                        /zkCleanup.sh
                                                                        /zkCli.sh
                                                                        /zkEnv.sh
                                                                        /zkServer.sh
                                    each calling somehow JAVA
                                    ex. zkCli.sh
                                             "$JAVA" "-Dzookeeper.log.dir=${ZOO_LOG_DIR}" "-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}" \
                                                     -cp "$CLASSPATH" $CLIENT_JVMFLAGS $JVMFLAGS \
                                                     org.apache.zookeeper.ZooKeeperMain "$@"





               $ srvr                to have zookeeper version
                                     possible output:
                                                 Zookeeper version: 3.4.6-1569965, built on 02/20/2014 09:09 GMT
                                                 Latency min/avg/max: 0/0/0
                                                 Received: 1
                                                 Sent: 0
                                                 Connections: 1
                                                 Outstanding: 0
                                                 Zxid: 0x0
                                                 Mode: standalone
                                                 Node count: 4


               It allows distributed applications (on different environments/servers)
               to coordindate with each other.

               This is done via a shared hierarchycal namespace which is organized
               similarly to a standard file system.
               (ex. a process p_3 of an APP2 could have the path   /APP2/p_3 )

               Applications use Zookeeper via TCP, sending requests
               getting responses and events.


   BASIS       ZooKeeper is very fast and very simple.
               It's thought simple to then build more complicated services,
               (ex. synchronization).

               So ZooKeeper provides a very simple programming interface, supporting only these API:

                          create          creates a node at a location in the tree
                          delete          deletes a node
                          exists          tests if a node exists at a location
                          get data        reads the data from a node
                          set data        writes data to a node
                          get children    retrieves a list of children of a node
                          sync            waits for data to be propagated





   REPLICATED  Like the distributed processes it coordinates, ZooKeeper itself
               is intended to be replicated over a sets of servers.

               These servers know of each other.

               Till the majority of the servers is running, Zookeeper can provide
               its service to the applications.

               Majority means half+1, so:
                   . the minimum 'good' number is TOTAL=3, because when TOTAL=2
                     if 1 stops even the other is suspended
                     while with TOTAL=5, it works if 2 go down

                   . always configure an ODD number of servers
                   . < 7
                     with >= 7 performance can start to degrade due to the nature of the consensus protocol.
                     So best is only 3 or 5



               Every time a TCP connection is lost, the application
               connects to another server.

               The servers use 2 TCP ports to achive their job:
                      . 1 to 'elect' the leader
                      . 1 to normally speak to each other

               Note that when a server is restarted after a fail, it joins without
               manual interaction to the other servers.


    ZNODE      ZNODE are like the Files of a File system.
               These 'files' are called 'node's or 'znode's.

               Every 'node' has information associated like:
                   . timpe stamps
                   . versions (for data, so a read will return also the data version and the client has this info)
                   . ACL (Access Control List) that restricts who can do what.


    WATCHES    Clients can set a WATCH on a znodes.
               A WATCH will be triggered and removed when the znode changes.


_______________ Installation & Usage


          CONFIG FILE         any name (ex conf/zoo.cfg)  #in our 3 AWS boxes is in /etc/zookeeper/conf.dist/zoo.cfg
                              example:

                                  tickTime=2000                             # heartbeats in ms
                                  dataDir=/var/lib/zookeeper                # location to store the in-memory database snapshots.
                                                                              In production, storage must be managed externally (not a different partition, but a different device)
                                              #in our 3 AWS boxes is same
                                                 # the directory where the snapshot is stored.
                                                 dataDir=/var/lib/zookeeper

                                  clientPort=2181                           # port to listen for client connections
                                  maxClientCnxns=60
                                  autopurge.snapRetainCount=3               # snapshots to retain in dataDir

                                  [dataLogDir]                              optional different place to store the log

                             It's passed when running the server in the list of the JAVA args
                             java -cp zookeeper.jar:lib/log4j-1.2.15.jar:conf \ org.apache.zookeeper.server.quorum.QuorumPeerMain zoo.cfg
                                                                                                                                  ^^^^^^^

             REPLICATION CONFIG adds these params

                      initLimit=5                   max time to wait to connect to a leader  ( is in 'ticks', so the time (in ms) is initLimit * tickTime, here 10 sec)
                      syncLimit=2                   max delay (also in 'ticks') of different dates a Sever has with the Leader
                      server.1=zoo1:2888:3888       . zoo?: hostname
                      server.2=zoo2:2888:3888       . 2888: port for Server-Server normal communication  (peerPort)
                      server.3=zoo3:2888:3888       . 3888: port for Leader election                     (leaderPort)

                            ^
                            these numbers 1,2,3 are actually the id of each server.
                            They must be integers (not important if 0 beased or sequential, just different integers)
                            For each server it's the number written as plain text
                            in the file <dataDir>/ myid     (the content is an integer with range 1-255)
                            nb. the filename is literally 'myid' (all lowercase)




   COMMANDS:     commands are shell scripts which call the JAR file.
                 Ex:
                      $JAVA" "-Dzookeeper.log.dir=${ZOO_LOG_DIR}" "-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}" \
                      cp "$CLASSPATH" $JVMFLAGS org.apache.zookeeper.server.upgrade.UpgradeMain ${@}



          START THE SERVER
                           ( no replication )       bin/zkServer.sh start

                              ( replication )       bin/zkServer.sh start     <---- same command
                                                                                    but if there are more Servers
                                                                                    It must be execute on each


          CONNECTING                                bin/zkCli.sh -server 127.0.0.1:2181
                                                    bin/zkCli.sh  <-----  also alone

                                                          this opens a CLI.
                                                          To see al the commands: "help"

          CLI                                       the CLI is zkCli.sh.
                                                    I can:
                                                      . prepend every command with ./zkCli.sh (ex. ./zkCli.sh create /stefano null)
                                                      . enter the CLI once (./zkCli.sh) and then run all commands (ex. create /stefano null)
                                                        (The CLI prompt is something like:
                                                                  [zk: localhost:2181(CONNECTED) 12]       12 is the history number of the command
                                                                                                           as with bash with 'history' I see the previous commands

         EXAMPLES
                   create 'files' (nodes):

                   1. create /stefano    null
                   2. create /stefano/basile  null
                   3. create /stefano/basile/is   null
                   4. create /stefano/basile/is/ok   "my data blah blah"  <---- nb. I need the previous 3 commands
                                                                         I cannot do in 1 command only: create /stefano/basile/is/ok "my data blah blah"

                   5. ls /           <---- like normal file system

                   6. get /stefano/basile/is/ok              will output:

                                                             my data blah blah       <----------
                                                             cZxid = 0x10d1
                                                             ctime = Fri Jul 27 14:35:08 BST 2018
                                                             mZxid = 0x10d1
                                                             mtime = Fri Jul 27 14:35:08 BST 2018
                                                             pZxid = 0x10d1
                                                             cversion = 0
                                                             dataVersion = 0      <----------- 0
                                                             aclVersion = 0
                                                             ephemeralOwner = 0x0
                                                             dataLength = 29
                                                             numChildren = 0
                                                             [zk: localhost:2181(CONNECTED) 23]

                   7. set /stefano/basile/is/ok  "new info now"         <----- I can change the data.
                                                                               n.b.  the version is increased: dataVersion = 1

                   8. delete /stefano/basile/is/ok



   STORAGE:
                  The ZooKeeper Data Directory contains files which are a persistent copy of the znodes.

                  As changes are made to the znodes these changes are appended to a transaction log.

                  A ZooKeeper server will not remove old snapshots and log files, this is the responsibility of the operator.


   LOG:          The LogFormatter class allows an administrator to look at the transactions in a log.



__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API
__________________________________________________  API















1. zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties
2. kafka-server-start     /usr/local/etc/kafka/server.properties


kafka-topics  --create --zookeeper localhost:2181 --replication-factor 1 --partitions 13 --topic my-topic
kafka-topics --list --zookeeper localhost:2181

kafka-console-producer --broker-list localhost:9092 --topic my-topic
kafka-console-consumer --bootstrap-server localhost:9092 --topic my-topic --from-beginning
—————————————————————————






WHY streaming:
   - pipeline sprawl
   - everything is syncronous


   - ALL your data is "EVENT-STREAMS"
   - DATA PIPELINE = EVENT STREAM  (keep a DB in sync: every DB update is streamed)

   - TOPIC = PARTITIONED LOG   (the huge LOG for scalability is partitioned in 'name - spaces'
   - 4 CORE APIs:
                  1. PRDUCER
                  2. CONSUMER
                  3. CONNECTORS
                                   KAFKA CONNECTOR DOES THE HARD WORK:
                                            - SCLAE OUT
                                            - FAULT TOLERANCE
                                            - CENTRAL MANAGEMENT
                                            - SCHEMA PROPAGATION
                  4. STREAMS
-----------------------------------------------------------------------------------------

HIGH TROUGHPUT :
                    - Scaled-out architecture:
                                                 - producers -- (send to) --> KAFKA Cluster : { many brokers := each broker { many TOPIC partitions} }


                    - SO, TOPIC PARTITION is the UNIT.
                      We assign it to brokers.

                    - PARALLELISM IS THEN POSSIBLE ---> HIGH TROUGHPUT = PARALLEL


                    - EACH PARTITION = PERSISTENT LOG

                    - LOG is an APPEND-ONLY BUFFER <-----
                      (each MESSAGE haa 1 UNIQUE ID (offset))
                       EVER GROWING)

                    - MESSAGE STRUCTURE:

                  OFFSET       MSG   CRC  MAGIC ATTRIBUTE        KEY     KEY      VALUE
                             LENGTH       BYTE       TIMESTAMP   LENGTH  CONTENT  LENGTH     VALUE CONTENT

            +---------------+------+------+---+---+-------------+------+-------+------+----------------------+
            |     8         |  4   |  4   | 1 | 1 |     8       |  4   | var   |  4   |      var             |
            +---------------+------+------+---+---+-------------+------+-------+------+----------------------+



                     MAGIC BYTE : is the KAFKA MSG version  : 0 / 1 (they added the timestamp)

                     ATTRIBUTR: if the message is compressed or not, and if compressed the codec)



                     PRODUCER -----/  /----> BROKER     -----/  /---> CONSUMER


                                    /  / : actually the interface to and from the BROKER are cached.
                                           - PRODUCER messages are cached into a BUFFER and then send all together to the BROKER
                                           - BROKER then will FLUSH them to the CONSUMER

                                           This is done in the background in batch processing jobs.
                                           The multi-messages buffer is compressed. This is better
                                           then compressing individual messages as it strips lots of message-commonalities.

                                           Compression also saves bandwith to the server and disk space



                    - REPLICATION FACTOR

                         The same UNIT (we know is 1 TOPIC PARTITION) is copied into different BROKERS

                         We want that every UNIT (1 TOPIC PARTITION) is copied in an even-number of other BROKERS

                         Every UNIT/TOPIC PARTITION has 1 of the BROKERS it is on, as LEADER BROKER
                         That BROKER will copy that UNIT to the other BROKERS

                         When a PRODUCER send that UNIT it must send to the LEADER
                         If it sends to a FOLLOWER broker, the message will be discarded.

                         The message will be exposed to the CONSUMER for consumption only
                         when the LEADER has received an ack from all the FOLLOWERS  (acks = all)
                         (acks = 0    not wait for all REPLICA acks before answering ack to the PRIDUCER)
                         (acks = 1    not wait for 1   REPLICA ack  before answering ack to the PRIDUCER)


                         IN-SYNC REPLICA SET: is the SET of all the REPLICAS actually fully equipped
                                              with ALL the messages.
                                              If 1 REPLICA felt down and now is UP but not fully up-dated
                                              with ALL the messages it lost in the downtime, (i.e. it's at 70%
                                              of msg-retrieving), then even if UP it's not in the IN-SYNC REPLICA SET.
                                              This avoids it could become LEADER (if any other running
                                              LEADER REPLICA should now fail) before it's fully ready.


                         UNCLEAN LEADER ELECTION: it's the condition where even if 70%, but if that REPLICA
                                              at some point should be the only UP BROKER, then it will be set LEADER anyhow
                                              (even if missing the 30% of downtime lost messages)

                         MIN.INSYNC.REPLICAS: if is 1, it means that even with 1 only single UP-100% REPLICA (so for sure also LEADER)
                                              I accept to store and commit an ack to the producer (even if I cannot copy to
                                              any other FOLLOWER)

                         MISSION CRITICAL:
                                              unclean.leader.election.enable = false
                                              default.replication.factor = 3
                                              min.insync.replicas = 2
                                              acks = all

                         CONTROL BROKER: is the broker elected betwen the live ones to process the liveness
                                         of the replicas. It keeps controlling the status of all to always know
                                         which is UP and which is DOWN.


                         LOG COMPACTION: is the broker elected betwen the live ones to process the liveness
                                         of the replicas. It keeps controlling the status of all to always know
                                         which is UP and which is DOWN.











--------------------------------------------------------------------------------
   Kafka: The Definitive Guide

                          (APACHE) KAFKA started at Linkedin

                          Released OPEN SOURCE on GitHub in 2010.   (JAVA code)

                          Accepted as an Apache Software Foundation incubator project in July of 2011.

                          Graduated from the incubator in October 2012.


        KAFKA moves data around.
        It solves the problem to dispatch the data as quick as possible
        from a PRODUCER to a CONSUMER.
        (which is a recurrent issue/pattern in many modern
        corporate architectures (DATA-DRIVEN enterprise).

        Relying on an efficient & fast dispatch messaging layer
        I can focus on the core logic of my architecture.

        Decouples PRODUCERS and CONSUMERS by using a push-pull model





        continuous data STREAMS  --> Kafka =  STREAMING platform   = you publish and subscribe to STREAMS of data


        90 % is like MESSAGEs system but with these 3 differences:

                  1. DISTRIBUTED system ( runs as a CLUSTER and
                                          can SCALE )

                  2. true STORAGE system ( data are : REPLICATED,
                                                      PERSISTENT for as long as you might like )

                  3. ABSTRACTION : the streamed contents can be computed and interpreted in any way
                                   not only as just handing out messages.




        Kafkfa can be also thought as a
                          real-time version of Hadoop.

        Hadoop = big data system which lets you store and periodically process
                 file data at a very large scale.

                 It usually comes with added softwares for post-processing / batch processing
                 the analytical data. So the processing is postponed or high latency.

                 Kafka instead is low latency, as it's a continuos processing.


--------------------------------------------------------------------------------


            SENDER (publisher) of a piece of data (message)
                               not specifically directing it to a receiver.
                               It CLASSIFIES the message somehow.

            RECEIVER (subscriber) subscribes to receive
                               certain CLASSES of messages.

            Pub/sub systems often have a BROKER

            The UNIT of data within Kafka is called a MESSAGE
                        (simply an ARRAY of BYTES = data content has no meaning to Kafka)

            KEY: A message can have an optional bit of METADATA (the Key)
                 The key is also a BYTE ARRAY having
                 no specific meaning to Kafka.

                 KEYS are used to OPTIMISE when MESSAGES are to be written to PARTITIONS
                 Ex: generate a consistent HASH of the KEY ==>
                     Then select the PARTITION number for that HASH (
                     MESSAGES with = KEY --> same PARTITION.

            BATCH: is just a COLLECTION of messages, produced to the SAME (TOPIC & PARTITION)
                   (writing in groups is a TRADEOFF between
                            . latency
                            . throughput:
                    the LARGER the batch, the more messages can be handled per unit of time,
                    but the LONGER it takes an individual message to propagate.

                    Batches are also typically compressed,
                         PROS: more efficient data transfer & storage
                         CONS: at the cost of some processing power.


            SCHEMA: robust type handling
                    (usually Apache Avro:
                                    SERIALIZATION FRAMEWORK originally developed for Hadoop).

                                  . Provides a COMPACT serialization format;
                    ):

                    Importance of SCHEMAS:
                        it allows WRITING and READING messages to be decoupled.
                        The contract is the format defined by a well defined SCHEMA.

            TOPICS: They are like a Table in a DB, or a Slack CHANNEL

                    . 1 TOPIC = { MANY PARTITIONS }

                                . 1 PARTITION =
                                                . 1 LOG of COMMITS
                                                . APPEND-only
                                                . reads are in ORDER from start to end (
                                                  this occurs via an incremental OFFSET for each message)
                                                . Each PARTITION can be hosted on a DIFFERENT server,
                                                  which means that a single TOPIC can be SCALED HORIZONTALLY
                                                  across MULTIPLE SERVERS ( so PARTITIONS are the way Kafka
                                                  provides REDUNDANCY & SCALABILITY).

                                NO-ORDER-TOPIC:
                                                . reads are ORDERED per PARTITION only
                                                . as 1 TOPIC = + PARTITIONS, the ORDER of reads from a TOPIC
                                                  is not guaranteed


                             So the 3 fields:
                                               ( TOPIC, PARTITION, OFFSET )  identify  1 MESSAGE

.
                                                                                                          Writes here (APPENDED)
                              -------------------------------------------------------------------------------|---------------------
                              |       __________ __________ __________ __________ __________ __________ _____|____
                              |      |  msg 1   |  msg 2   |  msg 3   |  msg 4   |  msg 5   |  msg 6   |  msg 7   |   PARTITION 1
                        T     |      |__________|__________|__________|__________|__________|__________|__________|
                         O    |
                          P   |       __________ __________ __________ __________
                           I  |      |  msg 1   |  msg 2   |  msg 3   |  msg 4   |                                    PARTITION 2
                            C |      |__________|__________|__________|__________|
                              |
                              |       __________ __________ __________ __________ __________ __________
                              |      |  msg 1   |  msg 2   |  msg 3   |  msg 4   |  msg 5   |  msg 6   |              PARTITION 3
                              |      |____._____|__________|__________|__________|__________|__________|
                              |           |
                              ------------|---------------------------------------------------------------------------------------
                                          |
                                    Reads here (from start)


            CLIENTS: They are 4 types in Kafka:

                     2 BASIC:    . PRODUCER
                                 . CONSUMER

                     2 ADVANCED: They are built on top of the 2 BASICS
                                 They use PRODUCERS and CONSUMERS as building blocks to provide higher-level functionality
                                 . Kafka CONNECT for data integration
                                 . Kafka STREAMS for stream processing.



------------------------------

   PRODUCER:   Usually a message is produced to a specific TOPIC.
               The PARTITION is usually not specified, so Kafka will balance it EVENLY.
               Sometimes the PARTITION is specified by the PRODUCER according to the KEYS.
               (mapping HASK-KEY ---> PARTITION )  <---- note that so 1 PARTITION can map more KEYS of the same TOPIC




------------------------------

   CONSUMER:   The SUBSCRIBE to 1 or + TOPICs.


               By storing the OFFSET of the LAST consumed message for each PARTITION,
               either in Zookeeper or in Kafka itself, a CONSUMER can
                              STOP and RESTART without losing its place.


   CONSUMER GROUP: 1 or + CONSUMER to manage 1 TOPIC.
                   (So 1 GROUP ---> 1 TOPIC       but possible 1 same TOPIC consumed by + GROUPS)



                   CONSTRAINT : 1 PARTITION cannot be consumed by + members of the GROUP

                                1 PARTITION <-- 1 member

                                (The mapping of 1 member to a PARTITION is called OWNERSHIP of the PARTITION
                                 by the consumer).

               -----------------------------------------------------------------------------------------------------  GROUP (2 members)
               |                   __________ __________ __________ __________ __________ __________ __________   ------------
               |  PARTITION 1     |  msg 1   |  msg 2   |  msg 3   |  msg 4   |  msg 5   |  msg 6   |  msg 7   | |            |
         T     |                  |__________|__________|__________|__________|__________|__ \  ____|__________| |            |
          O    |                                                                              '--------------------CONSUMER 1 | ownership of 1 PARTITION
           P   |                   __________ __________ __________ __________                                   |            |
            I  |  PARTITION 2     |  msg 1   |  msg 2   |  msg 3   |  msg 4   |                                  |            |
             C |                  |__________|__________|__________|___ \ ____|                                  |            |
               |                                                         '-----------.-----------------------------CONSUMER 2 | ownership of 2 PARTITIONS
               |                   __________ __________ __________ __________ ___  / ____ __________            |            |
               |  PARTITION 3     |  msg 1   |  msg 2   |  msg 3   |  msg 4   |  msg 5   |  msg 6   |            |            |
               |                  |__________|__________|__________|__________|__________|__________|            --------------
               |
               ----------------------------------------------------------------------------------------------------



------------------------------

   BROKER: 1 SERVER
                                        . receives messages for a TOPIC
                   to PRODUCERS it      . commits them in 1 PARTITION
                                        . assigns their OFFSET

                   to CONSUMERS it      . fetches the specified message given (TOPIC / PARTITION / OFFSET)

           Usually a BROKER manages
                                    thousands of PARTITIONS
                                    millions  of messages / sec


------------------------------

   CLUSTER: + BROKERs


   CONTROLLER: Within a CLUSTER, 1 BROKER will also function as the CLUSTER CONTROLLER
               (elected AUTOMATICALLY from the LIVE members of the CLUSTER).

                 . Assigns PARTITIONS to BROKERS  (1 PARTITION --> 1 BROKER (this broker is the LEADER for that PARTITION))
                 . Monitors for broker FAILURES.

   REPLICATED PARTITIONS: Is when the same PARTITION is replicated, or otherwise, assigned to + BROKERS
                          (This provides REDUNDANCY).
                          1 BROKER at a time is the LEADER.

   MULTIPLE CLUSTERS: I can configure a Mirror of a CLUSTER to then be MULTI CLUSTER.
    (MirrorMaker)     This can be for many reasons, ex:
                           . Segregation of types of data
                           . Isolation for security requirements
                           . Multiple datacenters (disaster recovery)

                      This is achived through a Kafka tool called
                                 "MirrorMaker"
                      which is a Consumer-Producer" linked together
                      Messages are CONSUMED from one Kafka cluster and PRODUCED for another.


------------------------------

   RETENTION: policy to configure (on TOPIC base) till when the messages are kept in the PARTITION
              (whichever LIMIT is reached first):

              TIME: retaining messages for some PERIOD of time (e.g., 7 days) or
              SIZE: until the topic reaches a certain SIZE in bytes (e.g., 1 GB).

              KEY:  keep only the last message for a given KEY

         So RETENTION is configurable per TOPIC


--------------------------------------------------------------------------------
                      Chap 2.     INSTALLING KAFKA
__________________________________________________

    Apache Zookeeper:    used by Kafka for storing METADATA for the BROKERS.

    OPERATING SYSTEM:    being a JAVA appl. it can run on every OS
                         (but recommend Linux)

    JAVA:                JAVA (8) is then required.
                         Even if it's ok just the runtime JVM to execute
                         both Zookeeper and Kafka, to develop or extend
                         it's worth having
                         the full Java Development Kit (JDK)

    KAFKA:               After both
                             JAVA (>=8)
                             Zookeeeper
                         are installed, I can install Kafka


        _________________________
        i) Run Kafka server
        _________________________

          1) export JAVA_HOME= ....                                   ex export JAVA_HOME=/usr/java/jdk1.8.0_51
          2) kafka-server-start.sh -daemon ...../server.properties    ex usr/local/kafka/bin/kafka-server-start.sh -daemon \
                                                                                            /usr/local/kafka/config/server.properties


        _________________________
        ii) Create a topic:
        _________________________

          3) kafka-topics.sh  --create

             ex.
             # /usr/local/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test

             -> Created topic "test".

        _________________________
        iii) Verify a topic:
        _________________________

          4) kafka-topics.sh  --describe

             ex.
             # /usr/local/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic test

            -> Topic:test    PartitionCount:1    ReplicationFactor:1    Configs:
            -> Topic: test    Partition: 0    Leader: 0    Replicas: 0    Isr: 0


        _________________________
        iv) Produce msg to a topic:
        _________________________


          5) kafka-console-producer.sh

            ex:
            # /usr/local/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
            Test Message 1
            Test Message 2
            ^D
            #

        _________________________
        v) Consume msg from a topic:
        _________________________


          6) kafka-console-consumer.sh

            # /usr/local/kafka/bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
            Test Message 1
            Test Message 2
            ^C
            Consumed 2 messages #

















--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
